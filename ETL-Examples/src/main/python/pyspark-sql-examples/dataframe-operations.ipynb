{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department1:  Row(id='123456', name='Computer Science')\n",
      "employee2:  Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "email-id of first employee in departmentWithEmployees1:  no-reply@berkeley.edu\n",
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print('department1: ', department1)\n",
    "print('employee2: ', employee2)\n",
    "print('email-id of first employee in departmentWithEmployees1: ', departmentWithEmployees1.employees[0].email)\n",
    "\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "df1.show()\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF = df1.unionAll(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unionDF.write.parquet(\"file:/E:/code/git-2018/ETL-Workflow/ETL-Examples/src/main/python/resources/df-example.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF = spark.read.parquet(\"file:/E:/code/git-2018/ETL-Workflow/ETL-Examples/src/main/python/resources/df-example.parquet\")\n",
    "parquetDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|anInt|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).show()\n",
    "eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = parquetDF.select(explode(\"employees\").alias(\"e\"))\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "explodeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# Use `|` instead of `or`\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "whereDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF = explodeDF.fillna(\"--\")\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\n",
    "filterNonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------+\n",
      "|firstName|lastName|count(DISTINCT firstName)|\n",
      "+---------+--------+-------------------------+\n",
      "|     null| wendell|                        0|\n",
      "|    matei|    null|                        1|\n",
      "| xiangrui|    meng|                        1|\n",
      "|  michael|armbrust|                        1|\n",
      "+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "countDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n",
    "  .groupBy(\"firstName\", \"lastName\")\\\n",
    "  .agg(countDistinct(\"firstName\"))\n",
    "\n",
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+\n",
      "|firstName|lastName|distinct_first_names|\n",
      "+---------+--------+--------------------+\n",
      "|     null| wendell|                   0|\n",
      "|    matei|    null|                   1|\n",
      "| xiangrui|    meng|                   1|\n",
      "|  michael|armbrust|                   1|\n",
      "+---------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodeDF.registerTempTable(\"df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = spark.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM df_example GROUP BY firstName, lastName\")\n",
    "\n",
    "countDistinctDF_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|    1040000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\n",
    "salarySumDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|          130000.0|\n",
      "| stddev|23904.572186687874|\n",
      "|    min|            100000|\n",
      "|    max|            160000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodeDF.describe(\"salary\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
